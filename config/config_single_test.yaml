# distributed training
nodes: 1
gpus: 1 # I recommend always assigning 1 GPU to 1 node
nr: 0 # machine nr. in node (0 -- nodes - 1)
dataparallel: 0 # Use DataParallel instead of DistributedDataParallel
workers: 16
dataset_dir: "/hdd1/DeepFakes/FF++/raw/train/"
train_file: "../FaceForensicsImages/ff_train_raw.txt"

dataset_dir_test: "/hdd1/DeepFakes/FF++/c40/test/"
test_file: "../FaceForensicsImages/ff_test_raw.txt"

# train options
seed: 42 # sacred handles automatic seeding when passed in the config
backbone: 'cspdarknet53'  # Can be resnet50, resnet101, cspdarknet53, efficientnet (b4)
batch_size: 18
image_size: 192
crop_size: 160
marginal: 3
heads: 4  # number of multihead, 0 means using gMLP
L1_depth: 1  # depth of transformer in spatial dim
L2_depth: 1  # depth of transformer in temporal dim
start_epoch: 0
epochs: 50
FRR: 16      # number of slices we sampled from CT
FREQ: 2      # The interval between frames/slices
MultiFREQ: "1,3"  # override the setting of FREQ for variant FREQ if not None
dataset: "FaceForensics" # STL10
pretrain: True
pos_weight: 0.25
centerCrop: 0 # [0~100] 0: off, >0 indicates how much percentage of slices will be kept for training
feature: 'xcsltp'
lbp_radius: 1
lbp_method: ''

view_interval: 20
lr: 0.0001

# model options
resnet: "VViT"
SSL: 'SimCLR'
n_features: 4096
# loss options
optimizer: "Adam" # or LARS (experimental)
weight_decay: 1.0e-6 # "optimized using LARS [...] and weight decay of 10âˆ’6"
temperature: 0.5 # see appendix B.7.: Optimal temperature under different batch sizes

# reload options
model_path: "checkpoint/ViTSingle-xcsltp" # set to the directory containing `checkpoint_##.tar` 
epoch_num: 49 # set to checkpoint number
reload: True

# logistic regression options
logistic_batch_size: 256
logistic_epochs: 1500
log_name: "simclr-single-xcsltp"
test_aug: True
max_det: 10
useFeatMap: -1
